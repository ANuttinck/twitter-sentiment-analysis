{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antoine NUTTINCK  \n",
    "MS Big Data  \n",
    "TELECOM ParisTech  \n",
    "05/06/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSE DES OPINIONS SOUS TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.12 |Anaconda custom (64-bit)| (default, Jun 29 2016, 11:07:13) [MSC v.1500 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sentiwordnet import SentiWordNetCorpusReader, SentiSynset\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join('.', 'testdata.manual.2009.06.14.csv')\n",
    "raw_data = pd.read_csv(data_path, sep='\",\"', header=None, engine='python',\n",
    "                           names=['polarite', 'id', 'date', 'requete',\n",
    "                                  'utilisateur', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implémentation d'un systèmede détection d'opinions dans les tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Matériel :  Présentation du corpus de tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Prétraitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretraitement(raw_tweets, dicoSlang):\n",
    "    \n",
    "    \n",
    "    tweets = []\n",
    "    url_pat = r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?'\n",
    "    nb_hashtags = 0\n",
    "    nb_ats = 0\n",
    "\n",
    "    for text in raw_tweets:\n",
    "        text_without_url = re.sub(url_pat, '', text)\n",
    "        text_without_url_ht, nb_hashtag = re.subn('#', '', text_without_url)\n",
    "        text_without_url_ht_at, nb_at = re.subn('@\\w*', '', text_without_url_ht)\n",
    "        words = nltk.word_tokenize(text_without_url_ht_at)\n",
    "        nb_hashtags += nb_hashtag\n",
    "        nb_ats += nb_at\n",
    "\n",
    "        for abr in dicoSlang.keys():\n",
    "            text_without_url_ht_at_abr = re.sub(abr, dicoSlang[abr], text_without_url_ht)\n",
    "        \n",
    "        clean_words = nltk.word_tokenize(text_without_url)\n",
    "    \n",
    "        tweets.append(clean_words)\n",
    "\n",
    "    print(\"Nombre de #\", nb_hashtags)\n",
    "    print(\"Nombre de @\", nb_ats)\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dico(dico_path):\n",
    "\n",
    "    with open(dico_path, 'rb') as dataFile:\n",
    "        dicoSlang = csv.reader(dataFile, delimiter='\\t')\n",
    "        if len(next(dicoSlang)) > 1:\n",
    "            dico = {}\n",
    "            for abrev in dicoSlang:\n",
    "                dico[abrev[0]] = abrev[1]\n",
    "            # dico = pd.read_csv(dico_path, delimiter='\\t+', index_col=0, encoding='cp1252', header=None, engine='python')\n",
    "        else:\n",
    "            dico = set()\n",
    "            dico.update(itertools.chain.from_iterable(dicoSlang))\n",
    "        dataFile.close()\n",
    "        \n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nombre de #', 52)\n",
      "('Nombre de @', 128)\n"
     ]
    }
   ],
   "source": [
    "dicoSlang = load_dico(os.path.join('.', 'SlangLookupTable.txt'))\n",
    "clean_tweets = pretraitement(raw_data.text, dicoSlang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Etiquetage grammatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_tagger(tweets):\n",
    "\n",
    "    taggedtweets = []\n",
    "    for tweet in tweets:\n",
    "        taggedtweets.append(nltk.pos_tag(tweet))\n",
    "\n",
    "    return taggedtweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 1104 mots étiquetés comme verbes dans le corpus.\n"
     ]
    }
   ],
   "source": [
    "tagged_tweets = pos_tagger(clean_tweets)\n",
    "cntvb = lambda tagged_tweet: len(filter(lambda pos: pos[1][:2] == 'VB', tagged_tweet))\n",
    "print(\"Il y a {0} mots étiquetés comme verbes dans le corpus.\".format(sum(map(cntvb, tagged_tweets))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Algorithme de détection v1 : appel au dictionnaire Sentiwordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj_tags = ['JJ', 'JJR', 'JJS']\n",
    "noun_tags = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "adv_tags = ['RB', 'RBR', 'RBS']\n",
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "accepted_tags = adj_tags + noun_tags + adv_tags + verb_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getting_sentiment(preprocessed_tweets, acceptedPoSList):\n",
    "    \n",
    "    swn_filename = 'SentiWordNet_3.0.0_20130122.txt'\n",
    "    swn = SentiWordNetCorpusReader(swn_filename)\n",
    "\n",
    "    sentimentScore = []\n",
    "\n",
    "    for pTweet in preprocessed_tweets:\n",
    "        posScore = 0\n",
    "        negScore = 0\n",
    "\n",
    "        for word, pos in pTweet:\n",
    "            if pos in acceptedPoSList:\n",
    "                synset = wn.synsets(word)\n",
    "                if synset:\n",
    "                    senti_synset = swn.senti_synset(synset[0].name())\n",
    "                    if senti_synset:\n",
    "                        posScore += senti_synset.pos_score\n",
    "                        negScore += senti_synset.neg_score\n",
    "        final_sent = '\"4\"' if posScore > negScore else '\"0\"' if posScore < negScore else '\"2\"'\n",
    "        sentimentScore.append([posScore, negScore, final_sent])\n",
    "\n",
    "    return sentimentScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_scores(predictions, labels):\n",
    "    \n",
    "    neg_right_preds = ((predictions == '\"0\"') * (labels == '\"0\"'))\n",
    "    true_neg = float(np.sum(neg_right_preds)) / np.sum((labels == '\"0\"'))\n",
    "    \n",
    "    pos_right_preds = ((predictions == '\"4\"') * (labels == '\"4\"'))\n",
    "    true_pos = float(np.sum(pos_right_preds)) / np.sum((labels == '\"4\"'))\n",
    "    \n",
    "    neut_right_preds = ((predictions == '\"2\"') * (labels == '\"2\"'))\n",
    "    true_neut = float(np.sum(neut_right_preds)) / np.sum((labels == '\"2\"'))\n",
    "    \n",
    "    return true_pos, true_neg, true_neut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 64.84% de tweets positifs détectés avec cette version de l'algorithme.\n",
      "Matrice de confusion:\n",
      "[[ 70  33  74]\n",
      " [ 14  62  63]\n",
      " [ 28  36 118]]\n",
      "Accuracy: 50.20%\n"
     ]
    }
   ],
   "source": [
    "preds = getting_sentiment(tagged_tweets, accepted_tags)\n",
    "valid_preds = compute_scores(np.array(preds)[:,2], raw_data.polarite.values)\n",
    "print(\"Il y a {0:.2%} de tweets positifs détectés avec cette version de l'algorithme.\".format(valid_preds[0]))\n",
    "print \"Matrice de confusion:\\n\", confusion_matrix(raw_data.polarite.values, np.array(preds)[:,2])\n",
    "print \"Accuracy: {:.2%}\".format(accuracy_score(raw_data.polarite.values, np.array(preds)[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Algorithme de détection v2 : gestion de la négation et des modifieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getting_sentiment_v2(raw_tweets,\n",
    "                            preprocessed_tweets,\n",
    "                            acceptedPoSList,\n",
    "                            negList,\n",
    "                            boosterWordList):\n",
    "    \n",
    "    swn_filename = 'SentiWordNet_3.0.0_20130122.txt'\n",
    "    swn = SentiWordNetCorpusReader(swn_filename)\n",
    "    negList_re = \"|\".join(map(lambda neg: re.escape(neg), list(negList)))\n",
    "    boostWdList_re = \"|\".join(map(lambda bstwd: re.escape(bstwd), list(boosterWordList)))\n",
    "\n",
    "    sentimentScore = []\n",
    "    # le facteur w permet d'augmenter l'impact du score d'un mot sur le score global du tweet\n",
    "    w = 1\n",
    "\n",
    "    for raw_tweet, pTweet in zip(raw_tweets, preprocessed_tweets):\n",
    "        posScore = 0\n",
    "        negScore = 0\n",
    "\n",
    "        for word, pos in pTweet:\n",
    "            if pos in acceptedPoSList:\n",
    "                synset = wn.synsets(word)\n",
    "                if synset:\n",
    "                    senti_synset = swn.senti_synset(synset[0].name())\n",
    "                    if senti_synset:\n",
    "                        # la structure du code permet qu'un mot soit à a la fois\n",
    "                        # un modifieur et une négation\n",
    "                        # (même si  ce n'est pas le cas ici)\n",
    "                        # if any([pTweet[n_tok - len(tbwd)].lower() for tbwd in tok_boosterWordList]):\n",
    "                        if re.search(\"(\" + negList_re + \")\\s*(\" + re.escape(word) + \")\", raw_tweet.lower()):\n",
    "                            w = 2\n",
    "\n",
    "                        # if any([sum([pTweet[n_tok - len(tnwd):n_tok]], []) == tnwd for tnwd in tok_negList]):\n",
    "                        if re.search(\"(\" + negList_re + \")\\s*(\" + re.escape(word) + \")\", raw_tweet.lower()):\n",
    "                            # print(re.search(\"(\" + negList_re + \")\\s*(\" + re.escape(word) + \")\", raw_tweet).groups())\n",
    "                            posScore += w * senti_synset.neg_score\n",
    "                            negScore += w * senti_synset.pos_score\n",
    "                        else:\n",
    "                            posScore += w * senti_synset.pos_score\n",
    "                            negScore += w * senti_synset.neg_score\n",
    "            w = 1\n",
    "        final_sent = '\"4\"' if posScore > negScore else '\"0\"' if posScore < negScore else '\"2\"'\n",
    "        sentimentScore.append([posScore, negScore, final_sent])\n",
    "\n",
    "    return sentimentScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 65.38% de tweets positifs correctement détectés avec la version 2 de l'algorithme.\n",
      "Il y a 40.11% tweets négatifs correctement détectés avec la version 2 de l'algorithme.\n",
      "Il y a 44.60% tweets neutres correctement détectés avec la version 2 de l'algorithme.\n",
      "\n",
      "Il y a 13/182 termes négatifs contenus dans les 182 tweets positifs.\n",
      "Matrice de confusion:\n",
      "[[ 71  31  75]\n",
      " [ 14  62  63]\n",
      " [ 27  36 119]]\n",
      "Accuracy: 50.60%\n"
     ]
    }
   ],
   "source": [
    "negList_filename = os.path.join('.', 'NegatingWordList.txt')\n",
    "negList = load_dico(negList_filename)\n",
    "negList_re = \"|\".join(map(lambda neg: re.escape(neg), list(negList)))\n",
    "\n",
    "booster_filename = os.path.join('.', 'BoosterWordList.txt')\n",
    "boosterWordList = load_dico(booster_filename).keys()\n",
    "\n",
    "preds2 = getting_sentiment_v2(raw_data.text,\n",
    "                                tagged_tweets,\n",
    "                                accepted_tags,\n",
    "                                negList,\n",
    "                                boosterWordList)\n",
    "\n",
    "# pos_tweets = [tw for tw, cl in zip(tagged_tweets, scores2) if cl[2]=='\"4\"']\n",
    "pos_tweets = raw_data.text[raw_data.polarite == '\"4\"']\n",
    "cntpos = len(pos_tweets)\n",
    "\n",
    "tr_pos2, tr_neg2, tr_neut2 = compute_scores(np.array(preds2)[:,2], raw_data.polarite.values)\n",
    "\n",
    "print(\"Il y a {0:.2%} de tweets positifs correctement détectés avec la version 2 de l'algorithme.\".format(tr_pos2))\n",
    "print(\"Il y a {0:.2%} tweets négatifs correctement détectés avec la version 2 de l'algorithme.\".format(tr_neg2))\n",
    "print(\"Il y a {0:.2%} tweets neutres correctement détectés avec la version 2 de l'algorithme.\".format(tr_neut2))\n",
    "\n",
    "neg_inpostw = [len(re.findall(\"\\s*(\" + negList_re + \")\\s*\", pos_tweet)) for pos_tweet in pos_tweets]\n",
    "cntneg_inpostw = sum(neg_inpostw)\n",
    "print(\"\\nIl y a {0}/{1} termes négatifs contenus dans les {2} tweets positifs.\".format(cntneg_inpostw,\n",
    "                                                                                          len(neg_inpostw),\n",
    "                                                                                          cntpos))\n",
    "\n",
    "print \"Matrice de confusion:\\n\", confusion_matrix(raw_data.polarite.values, np.array(preds2)[:,2])\n",
    "print \"Accuracy: {:.2%}\".format(accuracy_score(raw_data.polarite.values, np.array(preds2)[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Algorithme de détection v3 : gestion des emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_emoticons(emoticons_filename):\n",
    "\n",
    "    emoticons_dico = load_dico(emoticons_filename)\n",
    "    pos_emoticons = []\n",
    "    neg_emoticons = []\n",
    "\n",
    "    for emo, label in emoticons_dico.items():\n",
    "        if int(label) > 0:\n",
    "            pos_emoticons.append(emo)\n",
    "        elif int(label) < 0:\n",
    "            neg_emoticons.append(emo)\n",
    "        else:\n",
    "            # les emoticons avec un score de 0 ne sont pas renvoyer\n",
    "            pass\n",
    "\n",
    "    return pos_emoticons, neg_emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getting_sentiment_v3(raw_tweets,\n",
    "                            preprocessed_tweets,\n",
    "                            acceptedPoSList,\n",
    "                            negList,\n",
    "                            boosterWordList,\n",
    "                            posEmoticonList,\n",
    "                            negEmoticonList):\n",
    "    \n",
    "    swn_filename = 'SentiWordNet_3.0.0_20130122.txt'\n",
    "    swn = SentiWordNetCorpusReader(swn_filename)\n",
    "    negList_re = \"|\".join(map(lambda neg: re.escape(neg), list(negList)))\n",
    "    boostWdList_re = \"|\".join(map(lambda bstwd: re.escape(bstwd), list(boosterWordList)))\n",
    "    posEmos_re = \"|\".join(map(lambda p_emo: re.escape(p_emo), list(posEmoticonList)))\n",
    "    negEmos_re = \"|\".join(map(lambda n_emo: re.escape(n_emo), list(negEmoticonList)))\n",
    "    url_pattern = r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?'\n",
    "\n",
    "    sentimentScore = []\n",
    "    # le facteur w permet d'augmenter l'impact du score d'un mot sur le score global du tweet\n",
    "    w = 1\n",
    "\n",
    "    for raw_tweet, pTweet in zip(raw_tweets, preprocessed_tweets):\n",
    "        posScore = 0\n",
    "        negScore = 0\n",
    "\n",
    "        for word, pos in pTweet:\n",
    "            if pos in acceptedPoSList:\n",
    "                synset = wn.synsets(word)\n",
    "                if synset:\n",
    "                    senti_synset = swn.senti_synset(synset[0].name())\n",
    "                    if senti_synset:\n",
    "                        # la structure du code permet qu'un mot soit à a la fois\n",
    "                        # un modifieur et une négation\n",
    "                        # (même si  ce n'est pas le cas ici)\n",
    "                        if re.search(\"(\" + negList_re + \")\\W+(\" + re.escape(word) + \")\", raw_tweet.lower()):\n",
    "                            w = 2\n",
    "\n",
    "                        if re.search(\"(\" + negList_re + \")\\W+(\" + re.escape(word) + \")\", raw_tweet.lower()):\n",
    "                            posScore += w * senti_synset.neg_score\n",
    "                            negScore += w * senti_synset.pos_score\n",
    "                        else:\n",
    "                            posScore += w * senti_synset.pos_score\n",
    "                            negScore += w * senti_synset.neg_score\n",
    "            w = 1\n",
    "\n",
    "\n",
    "        nb_posemo = len(re.findall(\"(\" + posEmos_re + \")\\s*\", re.sub(url_pattern, \"\", raw_tweet)))\n",
    "        posScore += nb_posemo\n",
    "        nb_negemo = len(re.findall(\"(\" + negEmos_re + \")\\s*\", re.sub(url_pattern, \"\", raw_tweet)))\n",
    "        negScore += nb_negemo\n",
    "        \n",
    "        final_sent = '\"4\"' if posScore > negScore else '\"0\"' if posScore < negScore else '\"2\"'\n",
    "        sentimentScore.append([posScore, negScore, final_sent])\n",
    "\n",
    "    return sentimentScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 69.23% tweets positifs correctement détectés avec la version 3 de l'algorithme.\n",
      "Il y a 45.76% tweets négatifs correctement détectés avec la version 3 de l'algorithme.\n",
      "Il y a 44.60% tweets neutres correctement détectés avec la version 3 de l'algorithme.\n",
      "\n",
      "Il y a 180 émoticons contenus dans le corpus de 498 tweets.\n",
      "\n",
      "Matrice de confusion:\n",
      "[[ 81  28  68]\n",
      " [ 14  62  63]\n",
      " [ 25  31 126]]\n",
      "Accuracy: 54.02%\n"
     ]
    }
   ],
   "source": [
    "negList_filename = os.path.join('.', 'NegatingWordList.txt')\n",
    "negList = load_dico(negList_filename)\n",
    "booster_filename = os.path.join('.', 'BoosterWordList.txt')\n",
    "boosterWordList = load_dico(booster_filename).keys()\n",
    "emoticons_filename = os.path.join('.', 'EmoticonLookupTable.txt')\n",
    "posEmoticonList, negEmoticonList = load_emoticons(emoticons_filename)\n",
    "\n",
    "preds3 = getting_sentiment_v3(raw_data.text, tagged_tweets,\n",
    "                                accepted_tags,\n",
    "                                negList,\n",
    "                                boosterWordList,\n",
    "                                posEmoticonList,\n",
    "                                negEmoticonList)\n",
    "\n",
    "tr_pos3, tr_neg3, tr_neut3 = compute_scores(np.array(preds3)[:,2], raw_data.polarite.values)\n",
    "nb_tweets = len(tagged_tweets)\n",
    "\n",
    "print(\"Il y a {0:.2%} tweets positifs correctement détectés avec la version 3 de l'algorithme.\".format(tr_pos3))\n",
    "print(\"Il y a {0:.2%} tweets négatifs correctement détectés avec la version 3 de l'algorithme.\".format(tr_neg3))\n",
    "print(\"Il y a {0:.2%} tweets neutres correctement détectés avec la version 3 de l'algorithme.\".format(tr_neut3))\n",
    "\n",
    "emoticons = posEmoticonList + negEmoticonList\n",
    "emoticon_re = \"|\".join(map(lambda emo: re.escape(emo), emoticons))\n",
    "nb_emoticons = raw_data.text.str.count(emoticon_re).sum()\n",
    "print(\"\\nIl y a {0} émoticons contenus dans le corpus de {1} tweets.\\n\".format(nb_emoticons,\n",
    "                                                                              nb_tweets))\n",
    "print \"Matrice de confusion:\\n\", confusion_matrix(raw_data.polarite.values, np.array(preds3)[:,2])\n",
    "print \"Accuracy: {:.2%}\".format(accuracy_score(raw_data.polarite.values, np.array(preds3)[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Votre version : v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 68 tweets avec une polarité negative et qui sont prédis comme positifs.\n",
      "\n",
      "11     \"@Karoli I firmly believe that Obama/Pelosi ha...\n",
      "14     \"dear nike, stop with the flywire. that shit i...\n",
      "33     \"Played with an android google phone. The slid...\n",
      "35     \"omg so bored &amp; my tattoooos are so itchy!...\n",
      "48     \"?Obama Administration Must Stop Bonuses to AI...\n",
      "49     \"started to think that Citi is in really deep ...\n",
      "54     \"annoying new trend on the internets:  people ...\n",
      "64     \"@morind45 Because the twitter api is slow and...\n",
      "79     \"Took the Graduate Field Exam for Computer Sci...\n",
      "87     \"Can we just go ahead and blow North Korea off...\n",
      "88     \"North Korea, please cease this douchebaggery....\n",
      "93     \"just got back from church, and I totally hate...\n",
      "94     \"Just got mcdonalds goddam those eggs make me ...\n",
      "103    \"\"\"The Republican party is a bunch of anti-abo...\n",
      "104    \"is Twitter's connections API broken? Some twe...\n",
      "138    \"It's unfortunate that after the Stimulus plan...\n",
      "140    \"@jdreiss oh yes but if GM dies it will only b...\n",
      "141    \"Time Warner cable is down again 3rd time sinc...\n",
      "142    \"I would rather pay reasonable yearly taxes fo...\n",
      "143    \"NOOOOOOO my DVR just died and I was only half...\n",
      "149    \"I hate Time Warner! Soooo wish I had Vios. Ca...\n",
      "150    \"Ahh...got rid of stupid time warner today &am...\n",
      "152    \"is being fucked by time warner cable. didnt k...\n",
      "170    \"Son has me looking at cars online.  I hate ca...\n",
      "176    \"luke and i got stopped walking out of safeway...\n",
      "212    \"@siratomofbones we tried but Time Warner wasn...\n",
      "213    \"OMG - time warner f'ed up my internet install...\n",
      "217    \"Naive Bayes using EM for Text Classification....\n",
      "221    \"@KarrisFoxy If you're being harassed by calls...\n",
      "222    \"Just blocked United Blood Services using Goog...\n",
      "                             ...                        \n",
      "286                      \"F*** up big, or go home - AIG\"\n",
      "302    \"Now I can see why Dave Winer screams about la...\n",
      "307    \"#wolfram Alpha SUCKS! Even for researchers th...\n",
      "314    \"arhh, It's weka bug. = =\"\" and I spent almost...\n",
      "328    \"Oooooooh... North Korea is in troubleeeee! ht...\n",
      "329    \"Wat the heck is North Korea doing!!??!! They ...\n",
      "331    \"I just realized we three monkeys in the white...\n",
      "334    \"ugh. the amount of times these stupid insects...\n",
      "336    \"Just got barraged by a horde of insects hungr...\n",
      "344    \"Colin Powell rocked yesterday on CBS. Cheney ...\n",
      "373    \"It is a shame about GM. What if they are forc...\n",
      "374    \"As u may have noticed, not too happy about th...\n",
      "377    \"I Will NEVER Buy a Government Motors Vehicle:...\n",
      "378    \"Having the old Coca-Cola guy on the GM board ...\n",
      "380    \"Give a man a fish, u feed him for the day. Te...\n",
      "384    \"is upset about the whole GM thing. life as i ...\n",
      "385    \"whoever is running time warner needs to be re...\n",
      "388    \"Cox or Time Warner?  Cox is cheaper and gets ...\n",
      "394    \"You guys see this?  Why does Time Warner have...\n",
      "396    \"RT @sportsguy33: New Time Warner slogan: \"\"Ti...\n",
      "420    \"@ Safeway. Place is a nightmare right now. Bu...\n",
      "455    \"I still love my Kindle2 but reading The New Y...\n",
      "457    \"Although today's keynote rocked, for every gr...\n",
      "458    \"@sheridanmarfil - its not so much my obsessio...\n",
      "461    \"Fuzzball is more fun than AT&amp;T ;P http://...\n",
      "462    \"Today is a good day to dislike AT&amp;T. Vote...\n",
      "468    \"Man I kinda dislike Apple right now. Case in ...\n",
      "481    \"@Iheartseverus we love you too and don't want...\n",
      "485    \"Monday already. Iran may implode. Kitchen is ...\n",
      "490    \"I just created my first LaTeX file from scrat...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Ex de tweets classe a tord comme negatifs\n",
    "neg_predAsPos = raw_data.text[(np.array(preds3)[:,2]=='\"4\"') & (raw_data.polarite =='\"0\"')]\n",
    "print(\"Il y a {} tweets avec une polarité negative et qui sont prédis comme positifs.\\n\".format(neg_predAsPos.shape[0]))\n",
    "print(neg_predAsPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_synmatch(nltk_tag):\n",
    "    if  nltk_tag.startswith(\"VB\"): \n",
    "        return 'v'\n",
    "    elif nltk_tag.startswith(\"JJ\") :\n",
    "        return \"a\"\n",
    "    elif nltk_tag.startswith(\"ADV\") :\n",
    "        return \"r\"\n",
    "    elif nltk_tag.startswith(\"N\"):\n",
    "        return \"n\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getting_sentiment_v4(raw_tweets,\n",
    "                            preprocessed_tweets,\n",
    "                            acceptedPoSList,\n",
    "                            negList,\n",
    "                            boosterWordList,\n",
    "                            posEmoticonList,\n",
    "                            negEmoticonList):\n",
    "    \n",
    "    swn_filename = 'SentiWordNet_3.0.0_20130122.txt'\n",
    "    swn = SentiWordNetCorpusReader(swn_filename)\n",
    "    negList_re = \"|\".join(map(lambda neg: re.escape(neg), list(negList)))\n",
    "    boostWdList_re = \"|\".join(map(lambda bstwd: re.escape(bstwd), list(boosterWordList)))\n",
    "    posEmos_re = \"|\".join(map(lambda p_emo: re.escape(p_emo), list(posEmoticonList)))\n",
    "    negEmos_re = \"|\".join(map(lambda n_emo: re.escape(n_emo), list(negEmoticonList)))\n",
    "    url_pattern = r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?'\n",
    "\n",
    "    sentimentScore = []\n",
    "    # le facteur w permet d'augmenter l'impact du score d'un mot sur le score global du tweet\n",
    "    w = 1\n",
    "\n",
    "    for raw_tweet, pTweet in zip(raw_tweets, preprocessed_tweets):\n",
    "        posScore = 0\n",
    "        negScore = 0\n",
    "\n",
    "        for word, pos in pTweet:\n",
    "            if pos in acceptedPoSList:\n",
    "                synsets = wn.synsets(word, pos=tag_synmatch(pos))\n",
    "                if synsets:\n",
    "                    for synset in synsets[:3]:\n",
    "                        senti_synset = swn.senti_synset(synset.name())\n",
    "                        if senti_synset:\n",
    "                            if re.search(r'(' + negList_re + ')\\s*(' + re.escape(word) + ')', raw_tweet.lower()) or \\\n",
    "                                re.match(r'[A-Z]{2,}', word):\n",
    "                                w = 2\n",
    "\n",
    "                            if re.search(\"(\" + negList_re + \")\\s*(\" + re.escape(word) + \")\", raw_tweet.lower()):\n",
    "                                posScore += w * senti_synset.neg_score\n",
    "                                negScore += w * senti_synset.pos_score\n",
    "                            else:\n",
    "                                posScore += w * senti_synset.pos_score\n",
    "                                negScore += w * senti_synset.neg_score\n",
    "            w = 1\n",
    "\n",
    "\n",
    "        nb_posemo = len(re.findall(\"(\" + posEmos_re + \")\\s*\", re.sub(url_pattern, \"\", raw_tweet)))\n",
    "        posScore += nb_posemo\n",
    "        nb_negemo = len(re.findall(\"(\" + negEmos_re + \")\\s*\", re.sub(url_pattern, \"\", raw_tweet)))\n",
    "        negScore += nb_negemo\n",
    "        \n",
    "        final_sent = '\"4\"' if posScore > negScore else '\"0\"' if posScore < negScore else '\"2\"'\n",
    "        sentimentScore.append([posScore, negScore, final_sent])\n",
    "\n",
    "    return sentimentScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 84.07% tweets positifs correctement détectés avec la version 4 de l'algorithme.\n",
      "Il y a 40.11% tweets négatifs correctement détectés avec la version 4 de l'algorithme.\n",
      "Il y a 20.14% tweets neutres correctement détectés avec la version 4 de l'algorithme.\n",
      "Matrice de confusion:\n",
      "[[ 71  18  88]\n",
      " [ 23  28  88]\n",
      " [ 16  13 153]]\n",
      "Accuracy: 50.60%\n"
     ]
    }
   ],
   "source": [
    "negList_filename = os.path.join('.', 'NegatingWordList.txt')\n",
    "negList = load_dico(negList_filename)\n",
    "booster_filename = os.path.join('.', 'BoosterWordList.txt')\n",
    "boosterWordList = load_dico(booster_filename).keys()\n",
    "emoticons_filename = os.path.join('.', 'EmoticonLookupTable.txt')\n",
    "posEmoticonList, negEmoticonList = load_emoticons(emoticons_filename)\n",
    "\n",
    "preds4 = getting_sentiment_v4(raw_data.text,\n",
    "                                tagged_tweets,\n",
    "                                accepted_tags,\n",
    "                                negList,\n",
    "                                boosterWordList,\n",
    "                                posEmoticonList,\n",
    "                                negEmoticonList)\n",
    "\n",
    "tr_pos3, tr_neg3, tr_neut3 = compute_scores(np.array(preds4)[:,2], raw_data.polarite.values)\n",
    "\n",
    "print(\"Il y a {0:.2%} tweets positifs correctement détectés avec la version 4 de l'algorithme.\".format(tr_pos3))\n",
    "print(\"Il y a {0:.2%} tweets négatifs correctement détectés avec la version 4 de l'algorithme.\".format(tr_neg3))\n",
    "print(\"Il y a {0:.2%} tweets neutres correctement détectés avec la version 4 de l'algorithme.\".format(tr_neut3))\n",
    "print \"Matrice de confusion:\\n\", confusion_matrix(raw_data.polarite.values, np.array(preds4)[:,2])\n",
    "print \"Accuracy: {:.2%}\".format(accuracy_score(raw_data.polarite.values, np.array(preds4)[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
